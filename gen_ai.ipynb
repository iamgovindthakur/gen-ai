{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3df2b4f-ea0e-4eb3-9c57-629bc3c06de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "class OllamaLLM(LLM):\n",
    "    def __init__(self, api_url: str):\n",
    "        super().__init__()\n",
    "        self.api_url = api_url\n",
    "\n",
    "    def _call(self, prompt: str, stop=None):\n",
    "        # Send a request to the Ollama API\n",
    "        response = requests.post(\n",
    "            self.api_url,\n",
    "            json={\"prompt\": prompt},\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"response\"]\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"ollama\"\n",
    "\n",
    "# Replace with your local Ollama endpoint\n",
    "ollama_api_url = \"http://localhost:11434/api/v1/completions\"  # Example endpoint\n",
    "\n",
    "# Initialize your custom Ollama LLM\n",
    "llm = OllamaLLM(api_url=ollama_api_url)\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"book\"],\n",
    "    template=\"Name the author of the book {book}?\",\n",
    ")\n",
    "\n",
    "# Create the chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template, verbose=True)\n",
    "\n",
    "# Run the chain\n",
    "print(chain.run(\"The Da Vinci Code\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedef8ff-33e1-46cd-86c5-b2ad90a1915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ValueError: \"OllamaLLM\" object has no field \"api_url\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046c6e94-5c85-4e0f-805c-8fea39d6d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.1\")\n",
    "response = llm.invoke(\"The first man on the moon was ...\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d161f588-d085-4c91-af5a-5d20d72f8687",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -v langchain_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9fd4d2-ef52-4796-914a-df42fe539bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Initialize the Ollama LLM\n",
    "llm = OllamaLLM(model=\"llama3.3\")  # Replace with your Ollama model name\n",
    "\n",
    "# Define a prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain the topic: {topic}\",\n",
    ")\n",
    "\n",
    "# Create a chain with the LLM and prompt\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template, verbose=True)\n",
    "\n",
    "# Run the chain\n",
    "response = chain.run(\"Artificial Intelligence\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0544f5c4-1f90-4f0b-958a-6c37dcd6dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Initialize the Ollama LLM\n",
    "llm = OllamaLLM(model=\"llama3.1\")  # Replace with your Ollama model name\n",
    "\n",
    "print(\"Chat with Ollama! Type 'exit' to end the chat.\\n\")\n",
    "\n",
    "# Start the chat loop\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    \n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Ending the chat. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Generate a response from the model\n",
    "    try:\n",
    "        response = llm.invoke(user_input)\n",
    "        print(f\"Ollama: {response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d0e471-6c5a-4eb3-9bcb-ac75c496073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to SQLite database (creates it if it doesn't exist)\n",
    "conn = sqlite3.connect(\"my_database.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the employee table\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS employee (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    name TEXT NOT NULL,\n",
    "    position TEXT NOT NULL,\n",
    "    department TEXT NOT NULL,\n",
    "    salary REAL NOT NULL,\n",
    "    join_date TEXT NOT NULL\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Insert sample data into the employee table\n",
    "sample_data = [\n",
    "    (1, \"Alice Johnson\", \"Software Engineer\", \"IT\", 80000, \"2021-06-15\"),\n",
    "    (2, \"Bob Smith\", \"Data Analyst\", \"IT\", 70000, \"2019-08-10\"),\n",
    "    (3, \"Catherine Lee\", \"HR Manager\", \"HR\", 90000, \"2018-03-20\"),\n",
    "    (4, \"David Brown\", \"Marketing Specialist\", \"Marketing\", 60000, \"2022-01-05\"),\n",
    "    (5, \"Eva Green\", \"Software Engineer\", \"IT\", 85000, \"2020-11-11\")\n",
    "]\n",
    "\n",
    "cursor.executemany(\"\"\"\n",
    "INSERT INTO employee (id, name, position, department, salary, join_date)\n",
    "VALUES (?, ?, ?, ?, ?, ?)\n",
    "\"\"\", sample_data)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"Employee table created and populated with sample data!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ad56d-5b1a-4714-b4ca-725df1a0cea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Rebuild the SQLDatabaseChain class\n",
    "SQLDatabaseChain.model_rebuild()\n",
    "\n",
    "# Initialize database and LLM\n",
    "db = SQLDatabase.from_uri(\"sqlite:///my_database.db\")\n",
    "llm = OllamaLLM(model=\"llama3.1\")\n",
    "\n",
    "# Create the SQL Database Chain\n",
    "db_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True)\n",
    "\n",
    "# Run a natural language query\n",
    "response = db_chain.run(\"List all employees in the IT department earning more than 75000.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c561f6-1f15-472e-9b8d-0f637f86b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain==0.2.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d336b14-8e1b-47fd-88ef-3a0a9971f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "from langchain.sql_database import SQLDatabase\n",
    "\n",
    "# Initialize database connection\n",
    "db = SQLDatabase.from_uri(\"sqlite:///my_database.db\")\n",
    "\n",
    "# Initialize Ollama LLM (updated import)\n",
    "llm = OllamaLLM(base_url='http://localhost:11434', model=\"llama2\") # Replace llama2 with your model if needed\n",
    "\n",
    "# Create the SQL Database Chain\n",
    "db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True) # Use from_llm\n",
    "\n",
    "# Run a natural language query\n",
    "try:\n",
    "    response = db_chain.run(\"List all employees in the IT department earning more than 75000.\")\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b455928-2592-4555-8a84-4f1dfdd36b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Initialize the Ollama LLM\n",
    "llm = OllamaLLM(model=\"llama3.1\")  # Replace with your Ollama model name\n",
    "\n",
    "print(\"Chat with Ollama! Type 'exit' to end the chat.\\n\")\n",
    "\n",
    "# Sample schema for SQL\n",
    "schema = {\n",
    "    \"tables\": [\n",
    "        {\"name\": \"users\", \"columns\": [\"id\", \"name\", \"email\"]},\n",
    "        {\"name\": \"orders\", \"columns\": [\"id\", \"user_id\", \"product\", \"amount\"]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Function to generate SQL query from prompt\n",
    "def generate_sql(prompt, schema):\n",
    "    query_prompt = f\"Based on the schema: {schema}, create a SQL query: {prompt}\"\n",
    "    try:\n",
    "        response = llm.invoke(query_prompt)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Start the chat loop\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    \n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Ending the chat. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Generate SQL query based on the prompt\n",
    "    sql_query = generate_sql(user_input, schema)\n",
    "    print(f\"Ollama: {sql_query}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa2432b-f34f-422a-a6af-5d677a784dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall colorlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1383269e-4ecd-44c7-ad13-731273e94247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Initialize the Ollama LLM\n",
    "llm = OllamaLLM(model=\"llama3.1\")  # Replace with your Ollama model name\n",
    "\n",
    "print(\"Chat with Ollama! Type 'exit' to end the chat.\\n\")\n",
    "\n",
    "# Sample schema for SQL\n",
    "schema = {\n",
    "    \"tables\": [\n",
    "        {\"name\": \"trades\", \"columns\": [\"id\", \"trade_date\", \"symbol\", \"quantity\", \"price\", \"type\"]},\n",
    "        {\"name\": \"trade_details\", \"columns\": [\"trade_id\", \"order_id\", \"executed_price\", \"execution_time\"]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Function to generate SQL query from prompt\n",
    "def generate_sql(prompt, schema):\n",
    "    query_prompt = f\"Based on the schema: {schema}, create a SQL query: {prompt}\"\n",
    "    try:\n",
    "        response = llm.invoke(query_prompt)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Start the chat loop\n",
    "while True:\n",
    "    user_input = input(\"\\n\\n\\nYou: \")\n",
    "    \n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Ending the chat. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Generate SQL query based on the prompt\n",
    "    sql_query = generate_sql(user_input, schema)\n",
    "    print(f\"Ollama: {sql_query}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b7156-37f4-4b17-aadb-9c926d3af4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "import logging\n",
    "\n",
    "# Initialize the Ollama LLM\n",
    "llm = OllamaLLM(model=\"llama3.1\")  # Replace with your Ollama model name\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s: %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "print(\"Chat with Ollama! Type 'exit' to end the chat.\\n\")\n",
    "\n",
    "# Sample schema for SQL\n",
    "schema = {\n",
    "    \"tables\": [\n",
    "        {\"name\": \"users\", \"columns\": [\"id\", \"name\", \"email\"]},\n",
    "        {\"name\": \"orders\", \"columns\": [\"id\", \"user_id\", \"product\", \"amount\"]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Function to generate SQL query from prompt\n",
    "def generate_sql(prompt, schema):\n",
    "    query_prompt = f\"Based on the schema: {schema}, create a SQL query: {prompt}\"\n",
    "    try:\n",
    "        response = llm.invoke(query_prompt)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Start the chat loop\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    \n",
    "    if user_input.lower() == \"exit\":\n",
    "        logging.info(\"Ending the chat. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Generate SQL query based on the prompt\n",
    "    sql_query = generate_sql(user_input, schema)\n",
    "    if sql_query:\n",
    "        logging.info(f\"Ollama: {sql_query}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed32388-834a-4458-91b6-88c0eff52095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with Ollama! Type 'exit' to end the chat.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  find order of Govind\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 11:15:09 - INFO: HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2024-12-19 11:15:17 - INFO: Ollama: To find the order of Govind, I need to know which user has the name \"Govind\" and then retrieve their associated orders.\n",
      "\n",
      "After checking the database schema, I see that there is a table named 'users' with columns ['id', 'name', 'email'] and another table named 'orders' with columns ['id', 'user_id', 'product', 'amount']. \n",
      "\n",
      "To find Govind's order, I'll need to look up Govind in the users table by name ('name'), then look up their id, and finally retrieve all orders associated with that user id from the orders table.\n",
      "\n",
      "Here is the query:\n",
      "\n",
      "1. Select user id where user's name = \"Govind\"\n",
      "SELECT `id` FROM `users` WHERE `name` LIKE 'Govind'\n",
      "\n",
      "2. Once we get Govind's user id, we can select all his orders\n",
      "SELECT * FROM `orders` WHERE `user_id` = <get Govind's user id from step 1>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  give me name of users whose has highest order amount\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 11:22:17 - INFO: HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2024-12-19 11:22:31 - INFO: Ollama: To find the name of the user who has the highest order amount, we need to follow these steps:\n",
      "\n",
      "1. Select all orders with their corresponding `user_id` and calculate the total amount for each order.\n",
      "2. Group the orders by `user_id` and select the maximum amount for each group.\n",
      "3. Join the `users` table with the result from step 2 to get the name of the user who has the highest order amount.\n",
      "\n",
      "Here are the steps:\n",
      "\n",
      "1. Select all orders, calculate total amount, and get user id\n",
      "```sql\n",
      "SELECT \n",
      "    o.user_id,\n",
      "    SUM(o.amount) AS total_amount\n",
      "FROM \n",
      "    `orders` o\n",
      "GROUP BY \n",
      "    o.user_id\n",
      "```\n",
      "\n",
      "2. Join users table with result from step 1 to get name of the user who has highest order amount\n",
      "```sql\n",
      "SELECT \n",
      "    u.name, u.id\n",
      "FROM \n",
      "    `users` u\n",
      "JOIN (\n",
      "    SELECT \n",
      "        user_id,\n",
      "        MAX(total_amount) AS max_amount\n",
      "    FROM \n",
      "        (SELECT \n",
      "            o.user_id,\n",
      "            SUM(o.amount) AS total_amount\n",
      "        FROM \n",
      "            `orders` o\n",
      "        GROUP BY \n",
      "            o.user_id) t1\n",
      "    GROUP BY \n",
      "        user_id\n",
      ") t2 ON u.id = t2.user_id AND u.id = t2.max_amount\n",
      "```\n",
      "This will give us the name of the user who has the highest order amount.\n",
      "\n",
      "Note: The SQL query above assumes that we only want to consider users who have placed orders. If a user has not placed any orders, they will not be included in the result.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  any other way to achieve same result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 11:23:04 - INFO: HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2024-12-19 11:23:16 - INFO: Ollama: Yes, there is another way to achieve the same result using subqueries or window functions.\n",
      "\n",
      "Here are two alternative queries:\n",
      "\n",
      "**Method 1: Using a Subquery**\n",
      "\n",
      "```sql\n",
      "SELECT \n",
      "    u.name, u.id\n",
      "FROM \n",
      "    `users` u\n",
      "WHERE \n",
      "    u.id IN (\n",
      "        SELECT \n",
      "            user_id\n",
      "        FROM \n",
      "            (SELECT \n",
      "                o.user_id,\n",
      "                SUM(o.amount) AS total_amount\n",
      "            FROM \n",
      "                `orders` o\n",
      "            GROUP BY \n",
      "                o.user_id) t1\n",
      "        WHERE \n",
      "            total_amount = (SELECT MAX(total_amount) FROM (SELECT \n",
      "                o.user_id,\n",
      "                SUM(o.amount) AS total_amount\n",
      "            FROM \n",
      "                `orders` o\n",
      "            GROUP BY \n",
      "                o.user_id) t1)\n",
      "    )\n",
      "```\n",
      "\n",
      "**Method 2: Using Window Functions**\n",
      "\n",
      "```sql\n",
      "WITH ranked_orders AS (\n",
      "    SELECT \n",
      "        o.user_id,\n",
      "        SUM(o.amount) AS total_amount,\n",
      "        RANK() OVER (ORDER BY SUM(o.amount) DESC) AS rank\n",
      "    FROM \n",
      "        `orders` o\n",
      "    GROUP BY \n",
      "        o.user_id\n",
      ")\n",
      "SELECT \n",
      "    u.name, u.id\n",
      "FROM \n",
      "    `users` u\n",
      "JOIN ranked_orders ro ON u.id = ro.user_id\n",
      "WHERE \n",
      "    ro.rank = 1\n",
      "```\n",
      "\n",
      "Both of these queries will give you the name and ID of the user who has the highest order amount.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  will this command work on ibm db2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 11:23:40 - INFO: HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2024-12-19 11:23:56 - INFO: Ollama: The SQL commands I provided earlier use various features that might not be supported or might behave differently in IBM DB2.\n",
      "\n",
      "Here are some potential issues:\n",
      "\n",
      "1. `RANK()` function: This is a window function available in many databases, but its behavior and syntax might differ in DB2.\n",
      "2. Subqueries with aggregate functions: While subqueries with aggregate functions work in most databases, they might not be supported or may behave differently in DB2.\n",
      "3. Common Table Expressions (CTEs): The WITH clause I used in Method 2 is a CTE feature available in many databases, but its syntax and behavior might vary in DB2.\n",
      "\n",
      "However, the basic SQL queries should still work on IBM DB2, as they are based on standard SQL syntax.\n",
      "\n",
      "Here's an updated answer:\n",
      "\n",
      "The original query that I provided earlier:\n",
      "```sql\n",
      "SELECT \n",
      "    u.name, u.id\n",
      "FROM \n",
      "    `users` u\n",
      "JOIN (\n",
      "    SELECT \n",
      "        user_id,\n",
      "        MAX(total_amount) AS max_amount\n",
      "    FROM \n",
      "        (SELECT \n",
      "            o.user_id,\n",
      "            SUM(o.amount) AS total_amount\n",
      "        FROM \n",
      "            `orders` o\n",
      "        GROUP BY \n",
      "            o.user_id) t1\n",
      "    GROUP BY \n",
      "        user_id\n",
      ") t2 ON u.id = t2.user_id AND u.id = t2.max_amount\n",
      "```\n",
      "This query should work on IBM DB2.\n",
      "\n",
      "The other two queries I provided (Method 1 and Method 2) might require modifications to work correctly in DB2. For example, you might need to use a different syntax for the `RANK()` function or replace it with a `DENSE_RANK()` function, which is available in DB2.\n",
      "\n",
      "If you encounter issues or unexpected behavior, I recommend testing these queries on a sample dataset using IBM DB2's built-in tools or SQL client, such as DB2 Control Center or DB2 CLP (Command-Line Processor).\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "import logging\n",
    "\n",
    "# Initialize the Ollama LLM\n",
    "llm = OllamaLLM(model=\"llama3.1\")  # Replace with your Ollama model name\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s: %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "print(\"Chat with Ollama! Type 'exit' to end the chat.\\n\")\n",
    "\n",
    "# Sample schema for SQL\n",
    "schema = {\n",
    "    \"tables\": [\n",
    "        {\"name\": \"users\", \"columns\": [\"id\", \"name\", \"email\"]},\n",
    "        {\"name\": \"orders\", \"columns\": [\"id\", \"user_id\", \"product\", \"amount\"]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Maintain conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Function to generate SQL query from prompt\n",
    "def generate_sql(prompt, schema, history):\n",
    "    # Combine history with the new prompt\n",
    "    full_prompt = (\n",
    "        f\"Based on the schema: {schema}, \"\n",
    "        f\"here is the previous conversation history:\\n{history}\\n\\n\"\n",
    "        f\"Now, answer this: {prompt}\"\n",
    "    )\n",
    "    try:\n",
    "        response = llm.invoke(full_prompt)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Start the chat loop\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    \n",
    "    if user_input.lower() == \"exit\":\n",
    "        logging.info(\"Ending the chat. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Add user input to history\n",
    "    conversation_history.append(f\"You: {user_input}\")\n",
    "    \n",
    "    # Generate SQL query based on the prompt\n",
    "    sql_query = generate_sql(user_input, schema, \"\\n\".join(conversation_history))\n",
    "    \n",
    "    if sql_query:\n",
    "        conversation_history.append(f\"Ollama: {sql_query}\")\n",
    "        logging.info(f\"Ollama: {sql_query}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
